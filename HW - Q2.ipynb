{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "racial-university",
   "metadata": {},
   "source": [
    "# 2 TD learning with function approximation (3 pts.)\n",
    "Consider the MDP (S, A, p, r, γ), where  \n",
    "&emsp;&emsp;• S = {1, 2, 3, 4, 5, 6, 7} is the state space;  \n",
    "&emsp;&emsp;• A = {A, B} is the action space;  \n",
    "&emsp;&emsp;• The transition probabilities are summarized in the matrices  \n",
    "\n",
    "![image.info](./pictures/p-matrices.png)\n",
    "\n",
    "&emsp;&emsp;• r(s, a) = 0 for all pairs (s, a) ∈ S × A.  \n",
    "&emsp;&emsp;• γ = 0.99\n",
    "\n",
    "The q-function computed by the two algorithms should be:\n",
    "&emsp;&emsp;qw(x, a) = sum(k=1, 15)(φk(x, a)wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hollywood-negotiation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T22:10:23.187963Z",
     "start_time": "2021-11-16T22:10:23.179984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from fractions import Fraction\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aggregate-medicine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T02:57:26.766181Z",
     "start_time": "2021-11-17T02:57:26.750221Z"
    }
   },
   "outputs": [],
   "source": [
    "# TD learning with function approximation\n",
    "\n",
    "class TDLearning(object):\n",
    "    \n",
    "    def __init__(self, algorithm='q-learning', steps=500, alpha=0.01, gamma=0.99):\n",
    "        \n",
    "        # States are [1, 2, 3, 4, 5, 6, 7]\n",
    "        self.state_space = np.arange(1, 8)\n",
    "        \n",
    "        # Initial state\n",
    "        self.current_state = 1\n",
    "        \n",
    "        # Action space is [A, B]\n",
    "        self.action_space = ['A', 'B']\n",
    "        \n",
    "        # Probability matrix A\n",
    "        self.p_a = np.zeros((7, 7))\n",
    "        self.p_a[:, 6] = 1\n",
    "        \n",
    "        # Probability matrix B\n",
    "        self.p_b = np.zeros((7, 7))\n",
    "        self.p_b[:, :6] = Fraction(1, 6)\n",
    "        self.p_b[:, 6] = 0\n",
    "        \n",
    "        # Reward values are 0 for all action pairs (s, a)\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Initialize number of timesteps, step-size and discount factor\n",
    "        self.steps = steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize feature vector A - rows: states, columns: features\n",
    "        self.f_a = np.zeros((7, 15))\n",
    "        for i in np.arange(7):\n",
    "            self.f_a[i, i] = 2\n",
    "        self.f_a[6, 6] = 1\n",
    "        self.f_a[:, 7] = 2\n",
    "        self.f_a[6, 7] = 1\n",
    "        \n",
    "        # Initialize feature vector B\n",
    "        self.f_b = np.zeros((7, 15))\n",
    "        for i in np.arange(7):\n",
    "            self.f_b[i, i + 8] = 1\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros((15, 1)) + 1\n",
    "        self.weights[6] = 10\n",
    "        \n",
    "        # Probability of taking actions\n",
    "        self.use_a = Fraction(1, 7)\n",
    "        self.use_b = Fraction(6, 7)\n",
    "        \n",
    "        \n",
    "    # Policy to follow - translation of p_a and p_b into conditions\n",
    "    def use_policy(self, state):\n",
    "        if np.random.rand() < self.use_a:\n",
    "            return 7, 0\n",
    "        else:\n",
    "            if state < 7:\n",
    "                return np.random.randint(1, 7), 1\n",
    "            else:\n",
    "                return 7, 1\n",
    "            \n",
    "    \n",
    "    def calculate_q(self, state, action):\n",
    "        if action == 0:\n",
    "            temp_q = [self.f_a[state-1, i] * self.weights[i][0] for i in np.arange(15)]\n",
    "            return np.sum(temp_q)\n",
    "        elif action == 1:\n",
    "            temp_q = [self.f_b[state-1, i] * self.weights[i][0] for i in np.arange(15)]\n",
    "            return np.sum(temp_q)\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        \n",
    "    def get_features(self, state, action):\n",
    "        if action == 0:\n",
    "            return self.f_a[state-1, :]\n",
    "        elif action == 1:\n",
    "            return self.f_b[state-1, :]\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Q-Learning iteration loop\n",
    "    def qlearning_iteration(self):\n",
    "        \n",
    "        # Run through N episodes\n",
    "        for i in tqdm.tnrange(self.steps):\n",
    "            \n",
    "            # Choose action using policy\n",
    "            next_state, action = self.use_policy(self.current_state)\n",
    "            \n",
    "            # Get current Q values\n",
    "            current_q = self.calculate_q(self.current_state, action)\n",
    "            \n",
    "            # Get the Q values at next step\n",
    "            q_alternative = np.max([self.calculate_q(next_state, i) for i in np.arange(2)])\n",
    "            \n",
    "            # Immediate reward R\n",
    "            new_reward = 0\n",
    "            \n",
    "            # TD-target\n",
    "            temp_target = new_reward + self.gamma*q_alternative - current_q\n",
    "            #print(self.alpha * temp_target * self.get_features(self.current_state, action))\n",
    "            \n",
    "            # Weight update\n",
    "            weight_update = self.alpha * temp_target * self.get_features(self.current_state, action)\n",
    "            weight_update = np.reshape(weight_update, (15, 1))\n",
    "            \n",
    "            # Assign the weights\n",
    "            temp_weights = self.weights + weight_update\n",
    "            self.weights = temp_weights\n",
    "            \n",
    "            # Do the action and take next one\n",
    "            self.current_state = next_state\n",
    "            \n",
    "        print(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "disciplinary-favorite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T02:57:27.422038Z",
     "start_time": "2021-11-17T02:57:27.347250Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-3e72989b04e6>:89: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tqdm.tnrange(self.steps):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a62281c153844499219dc7ca2d697cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1378    ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 9.92806569]\n",
      " [ 1.06586569]\n",
      " [ 1.058904  ]\n",
      " [ 1.        ]\n",
      " [ 1.058904  ]\n",
      " [ 1.08791496]\n",
      " [ 1.0296    ]\n",
      " [ 1.058904  ]\n",
      " [10.7829402 ]]\n"
     ]
    }
   ],
   "source": [
    "td_test = TDLearning()\n",
    "td_test.qlearning_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    " # For each step of the episode\n",
    "while True: \n",
    "\n",
    "    # Choose action with epsilon-greedy\n",
    "    action = self.epsilon_greedy(self.current_row, self.current_col)\n",
    "\n",
    "    # Get new state S\n",
    "    new_row, new_col = self.do_action(action, self.current_row, self.current_col)\n",
    "\n",
    "    # Immediate reward R\n",
    "    new_reward = self.rewards[new_row, new_col]\n",
    "\n",
    "    # Current Q\n",
    "    current_q = self.get_q(action, self.current_row, self.current_col)\n",
    "\n",
    "    # Q(S', a)\n",
    "    q_alternative = np.max([self.get_q(a, new_row, new_col) for a in self.actions])\n",
    "\n",
    "    # Q'\n",
    "    temp_q = current_q + self.alpha*(new_reward + self.gamma*q_alternative - current_q)\n",
    "\n",
    "    # Set the new Q value\n",
    "    self.set_q(action, self.current_row, self.current_col, temp_q)\n",
    "\n",
    "    # Update action and state values\n",
    "    self.current_row, self.current_col = self.do_action(action, self.current_row, self.current_col)\n",
    "\n",
    "    episode_timestep += 1\n",
    "    episode_total_reward += new_reward\n",
    "    episode_rewards.append(new_reward)\n",
    "\n",
    "    if (self.current_row == self.goal_row) & (self.current_col == self.goal_col):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
