{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "united-recording",
   "metadata": {},
   "source": [
    "## 1 The cliff problem (3 pts.)\n",
    "\n",
    "Consider the following problem. An agent must navigate the grid world represented in Fig. 1, where the\n",
    "grey area corresponds to a cliff that the agent must avoid. The goal state corresponds to the cell marked\n",
    "with a G, while the cell marked with an S corresponds to a starting state. At every step, the agent receives\n",
    "a reward of −1 except in the cliff region, where the reward is −100. Whenever the agent steps into a cliff\n",
    "state, its position is reset back to the start state. When the agent steps into the goal state, the episode ends.\n",
    "The agent has available four actions: up (U), down (D), left (L) and right (R), all of which move the agent\n",
    "deterministically in the corresponding direction.\n",
    "\n",
    "![image.info](./pictures/grid-world.png)\n",
    "\n",
    "In this question, you will compare the performance of SARSA and Q-learning on the cliff task. To do\n",
    "so, assume that the agent follows an ε-greedy policy, with ε = 0.15. Run both algorithms for 500 episodes, making sure that the Q-values for both methods are initialized to 0. Consider throughout that γ = 1 and use a step-size of α = 0.5.\n",
    "\n",
    "\n",
    "### Question 1. Compare:\n",
    "• The total reward in each episode for Q-learning and SARSA, plotting the two in a single plot.<br>\n",
    "• The resulting policy after the 500 episodes.<br><br>\n",
    "Comment any differences observed.<br><br>\n",
    "<b>Note<b>: To mitigate the effect of noise on the plot, perform multiple runs and average the result across\n",
    "runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-technician",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T11:48:01.269760Z",
     "start_time": "2021-11-12T11:48:01.230260Z"
    }
   },
   "source": [
    "## Sarsa pseudo code\n",
    "\n",
    "![image.info](./pictures/sarsa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gothic-slovenia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T12:17:49.683270Z",
     "start_time": "2021-11-12T12:17:48.090624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "future-bishop",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T13:14:05.141638Z",
     "start_time": "2021-11-12T13:14:05.111686Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-24f56eecee8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m \u001b[0mCliff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-24f56eecee8b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, episodes, gamma, nrows, ncols, epsilon, alpha)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m#print(self.q_up)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_down\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\camar\\documents\\jupyter\\pythonenvs\\tf_2.4.1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \"\"\"\n\u001b[1;32m-> 1188\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\camar\\documents\\jupyter\\pythonenvs\\tf_2.4.1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\camar\\documents\\jupyter\\pythonenvs\\tf_2.4.1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Sarsa Implementation\n",
    "\n",
    "class Cliff(object):\n",
    "    \n",
    "    def __init__(self, episodes=500, gamma=1, nrows=4, ncols=11, epsilon=0.15, alpha=0.5):\n",
    "        \n",
    "        if (ncols < 3) | (nrows < 2):\n",
    "            print('Input at least two rows and three columns')\n",
    "            return\n",
    "        \n",
    "        # Action space: A = {up, down, left, right}\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        \n",
    "        # Space size\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "\n",
    "        # Grid space - available states: S = {4*11 matrix}\n",
    "        self.states = np.zeros((nrows, ncols))\n",
    "        \n",
    "        # Current state, initial state (nrows-1, 0)\n",
    "        self.current_row = None\n",
    "        self.current_col = None\n",
    "        \n",
    "        # Goal state\n",
    "        self.goal_row = nrows-1\n",
    "        self.goal_col = ncols-1\n",
    "        \n",
    "        # Number of episodes\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        # Q values\n",
    "        self.q_up = None\n",
    "        self.q_down = None\n",
    "        self.q_left = None\n",
    "        self.q_right = None\n",
    "        \n",
    "        # Rewards: -1 for every action, except for cliff, where it is -100\n",
    "        self.rewards = np.zeros((nrows, ncols)) - 1\n",
    "        self.rewards[nrows-1, 1:ncols-1] = -100 \n",
    "\n",
    "        # Discount factor gamma\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Epsilon value for epsilon-greedy policy exploration\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Step size alpha for incremental value approximation\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Optimal policy\n",
    "        self.policy = None\n",
    "        \n",
    "        #print(self.states)\n",
    "        #print(self.rewards)\n",
    "        #print(self.q_up)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def initialize_agent(self):\n",
    "        self.current_row = self.nrows-1\n",
    "        self.current_col = 0\n",
    "    \n",
    "    def initialize_q(self):\n",
    "        # Q-values: arbitrarily set to 1, except for terminal state, where it will be zero\n",
    "        self.q_up = np.zeros((nrows, ncols)) + 1\n",
    "        self.q_up[nrows-1, ncols-1] = 0\n",
    "        \n",
    "        self.q_down = np.zeros((nrows, ncols)) + 1\n",
    "        self.q_down[nrows-1, ncols-1] = 0\n",
    "        \n",
    "        self.q_left = np.zeros((nrows, ncols)) + 1\n",
    "        self.q_left[nrows-1, ncols-1] = 0\n",
    "        \n",
    "        self.q_right = np.zeros((nrows, ncols)) + 1\n",
    "        self.q_right[nrows-1, ncols-1] = 0\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def epsilon_greedy(self, state_row, state_col):\n",
    "        next_action = None\n",
    "        if np.random.random() < self.epsilon_greedy:\n",
    "            next_action = np.random.randint(4)\n",
    "        else:\n",
    "            next_action = np.argmax([self.q_up[state_row, state_col],\n",
    "                                     self.q_down[state_row, state_col],\n",
    "                                     self.q_left[state_row, state_col],\n",
    "                                     self.q_right[state_row, state_col]])\n",
    "        return next_action\n",
    "    \n",
    "            \n",
    "    def go_up(self, state_row, state_col):\n",
    "        if self.current_row - 1 > 0:\n",
    "            return self.current_row - 1, self.current_col\n",
    "        return self.current_row, self.current_col\n",
    "\n",
    "\n",
    "    def go_down(self, state_row, state_col):\n",
    "        if self.current_row + 1 < self.nrows:\n",
    "            return self.current_row + 1, self.current_col\n",
    "        return self.current_row, self.current_col\n",
    "\n",
    "\n",
    "    def go_left(self, state_row, state_col):\n",
    "        if self.current_col - 1 > 0:\n",
    "            return self.current_row, self.current_col - 1\n",
    "        return self.current_row, self.current_col\n",
    "\n",
    "    def go_right(self, state_row, state_col):\n",
    "        if self.current_row + 1 < self.ncols:\n",
    "            return self.current_row, self.current_col + 1\n",
    "        return self.current_row, self.current_col\n",
    "\n",
    "\n",
    "    def do_action(self, action, state_row, state_col):\n",
    "        if action == 0:\n",
    "            return self.go_up()\n",
    "        elif action == 1:\n",
    "            return self.go_down()\n",
    "        elif action == 2:\n",
    "            return self.go_left()\n",
    "        elif action == 3:\n",
    "            return self.go_right()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def get_q(self, action, current_row, current_col):\n",
    "        if action == 0:\n",
    "            return self.q_up[current_row, current_col]\n",
    "        elif action == 1:\n",
    "            return self.q_down[current_row, current_col]\n",
    "        elif action == 2:\n",
    "            return self.q_left[current_row, current_col]\n",
    "        elif action == 3:\n",
    "            return self.q_right[current_row, current_col]\n",
    "        \n",
    "    def set_q(self, action, current_row, current_col, value):\n",
    "        if action == 0:\n",
    "            self.q_up[current_row, current_col] = value\n",
    "        elif action == 1:\n",
    "            self.q_down[current_row, current_col] = value\n",
    "        elif action == 2:\n",
    "            self.q_left[current_row, current_col] = value\n",
    "        elif action == 3:\n",
    "            self.q_right[current_row, current_col] = value\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def get_policy(self):\n",
    "        max_q = np.zeros((nrows, ncols))\n",
    "        for row in np.arange(self.nrows):\n",
    "            for col in np.arange(self.ncols):\n",
    "                max_q[row, col] = np.argmax(self.q_up[row, col],\n",
    "                                            self.q_down[row, col],\n",
    "                                            self.q_left[row, col],\n",
    "                                            self.q_right[row, col])\n",
    "        print(max_q)\n",
    "        return max_q\n",
    "            \n",
    "            \n",
    "    def sarsa_iteration(self):\n",
    "        \n",
    "        # Initialize the Q values\n",
    "        self.initialize_q()\n",
    "        \n",
    "        # Go through each episode:\n",
    "        for episode in np.arange(self.episodes):\n",
    "            \n",
    "            # State initialization\n",
    "            self.initialize_agent()\n",
    "            \n",
    "            # Choose action with epsilon-greedy (initial action)\n",
    "            action = self.epsilon_greedy(self.current_row, self.current_col)\n",
    "            \n",
    "            # For each step of the episode\n",
    "            while true:\n",
    "                # Get new state S\n",
    "                new_row, new_col = self.do_action(action, self.current_row, self.current_col)\n",
    "                \n",
    "                # Immediate reward R\n",
    "                new_reward = self.rewards[new_row, new_col]\n",
    "                \n",
    "                # Get new action\n",
    "                new_action = self.epsilon_greedy(new_row, new_col)\n",
    "                \n",
    "                # Current Q\n",
    "                current_q = self.get_q(action, self.current_row, self.current_col)\n",
    "                \n",
    "                # Q'\n",
    "                new_q = self.get_q(new_action, new_row, new_col)\n",
    "                \n",
    "                # Update Q value\n",
    "                temp_q = current_q + self.alpha*(new_reward + self.gamma*new_q - current_q)\n",
    "                \n",
    "                # Set the new Q value for this action\n",
    "                self.set_q(new_action, new_row, new_col, temp_q)\n",
    "                \n",
    "                # Update action and state values\n",
    "                self.do_action(new_action)\n",
    "                action = new_action\n",
    "                \n",
    "                if (self.current_row == self.goal_row) & (self.current_col == self.goal_col):\n",
    "                    break\n",
    "            \n",
    "        return\n",
    "        \n",
    "    \n",
    "Cliff(nrows=4, ncols=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-waters",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
